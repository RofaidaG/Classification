{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loans = pd.read_csv('lending-club-data.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = ['grade',              # grade of the loan\n",
    "            'term',               # the term of the loan\n",
    "            'home_ownership',     # home ownership status: own, mortgage or rent\n",
    "            'emp_length',         # number of years of employment\n",
    "           ]\n",
    "loans['safe_loans'] = loans['bad_loans'].apply(lambda x : +1 if x==0 else -1)\n",
    "\n",
    "target = 'safe_loans'\n",
    "loans = loans[features + [target]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>grade</th>\n",
       "      <th>term</th>\n",
       "      <th>home_ownership</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>safe_loans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>36 months</td>\n",
       "      <td>RENT</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C</td>\n",
       "      <td>60 months</td>\n",
       "      <td>RENT</td>\n",
       "      <td>&lt; 1 year</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C</td>\n",
       "      <td>36 months</td>\n",
       "      <td>RENT</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C</td>\n",
       "      <td>36 months</td>\n",
       "      <td>RENT</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>36 months</td>\n",
       "      <td>RENT</td>\n",
       "      <td>3 years</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  grade        term home_ownership emp_length  safe_loans\n",
       "0     B   36 months           RENT  10+ years           1\n",
       "1     C   60 months           RENT   < 1 year          -1\n",
       "2     C   36 months           RENT  10+ years           1\n",
       "3     C   36 months           RENT  10+ years           1\n",
       "4     A   36 months           RENT    3 years           1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_index = pd.read_json('module-8-assignment-2-train-idx.json')\n",
    "train_index.columns = ['indexvalue']\n",
    "train_idx = train_index.indexvalue.tolist()\n",
    "\n",
    "test_index = pd.read_json('module-8-assignment-2-test-idx.json')\n",
    "test_index.columns = ['indexvalue']\n",
    "test_idx = test_index.indexvalue.tolist()\n",
    "\n",
    "train_data = loans.iloc[train_idx]\n",
    "test_data = loans.iloc[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((37224, 5), (9284, 5), (122607, 5))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape, test_data.shape, loans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1    18748\n",
      "-1    18476\n",
      "Name: safe_loans, dtype: int64\n",
      "-1    4674\n",
      " 1    4610\n",
      "Name: safe_loans, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print train_data['safe_loans'].value_counts()\n",
    "print test_data['safe_loans'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def df_encode_data(data, categorical_type):\n",
    "    return pd.get_dummies(data[categorical_type])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "category = features\n",
    "\n",
    "X_train_vector = df_encode_data(train_data[features], features)\n",
    "X_test_vector = df_encode_data(test_data[features], features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# there is no numerical feature in this case\n",
    "X_train = X_train_vector\n",
    "X_test = X_test_vector\n",
    "y_train = train_data[target]\n",
    "y_test = test_data[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((37224, 25), (37224,), (9284, 25), (9284,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def intermediate_node_weighted_mistakes(labels_in_node, data_weights):\n",
    "    # Sum the weights of all entries with label +1\n",
    "    label_positive_index = list(np.where(labels_in_node == +1)[0])\n",
    "    #print +1, len(label_positive_index)\n",
    "    total_weight_positive = sum(data_weights[label_positive_index])\n",
    "    #total_weight_positive = sum(data_weights[np.where(labels_in_node == +1)])\n",
    "    \n",
    "    # Weight of mistakes for predicting all -1's is equal to the sum above\n",
    "    ### YOUR CODE HERE\n",
    "    weighted_mistakes_all_negative = total_weight_positive\n",
    "    \n",
    "    # Sum the weights of all entries with label -1\n",
    "    ### YOUR CODE HERE\n",
    "    label_negative_index = list(np.where(labels_in_node == -1)[0])\n",
    "    #print +1, len(label_negative_index)\n",
    "    total_weight_negative = sum(data_weights[label_negative_index])\n",
    "    #total_weight_negative = sum(data_weights[np.where(labels_in_node == -1)])\n",
    "    \n",
    "    # Weight of mistakes for predicting all +1's is equal to the sum above\n",
    "    ### YOUR CODE HERE\n",
    "    weighted_mistakes_all_positive = total_weight_negative\n",
    "    \n",
    "    #print total_weight_positive, total_weight_negative\n",
    "    # Return the tuple (weight, class_label) representing the lower of the two weights\n",
    "    #    class_label should be an integer of value +1 or -1.\n",
    "    # If the two weights are identical, return (weighted_mistakes_all_positive,+1)\n",
    "    ### YOUR CODE HERE\n",
    "    if weighted_mistakes_all_positive > weighted_mistakes_all_negative:\n",
    "        #print weighted_mistakes_all_negative, -1\n",
    "        return (weighted_mistakes_all_negative, -1)\n",
    "    else:\n",
    "        #print weighted_mistakes_all_positive, +1\n",
    "        return (weighted_mistakes_all_positive, +1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "example_labels = np.array([-1, -1, 1, 1, 1])\n",
    "example_data_weights = np.array([1., 2., .5, 1., 1.])\n",
    "if intermediate_node_weighted_mistakes(example_labels, example_data_weights) == (2.5, -1):\n",
    "    print 'Test passed!'\n",
    "else:\n",
    "    print 'Test failed... try again!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If the data is identical in each feature, this function should return None\n",
    "def best_splitting_feature(data, features, target, data_weights):\n",
    "    \n",
    "    # These variables will keep track of the best feature and the corresponding error\n",
    "    best_feature = None\n",
    "    best_error = float('+inf') \n",
    "    num_points = float(len(data))\n",
    "\n",
    "    # Loop through each feature to consider splitting on that feature\n",
    "    for feature in features:\n",
    "        \n",
    "        # The left split will have all data points where the feature value is 0\n",
    "        # The right split will have all data points where the feature value is 1\n",
    "        left_split = data[data[feature] == 0]\n",
    "        right_split = data[data[feature] == 1]\n",
    "        \n",
    "        #print feature, len(left_split), len(right_split)\n",
    "        \n",
    "        # Apply the same filtering to data_weights to create left_data_weights, right_data_weights\n",
    "        ## YOUR CODE HERE\n",
    "        left_data_weights = data_weights[list(np.where(data[feature] == 0)[0])]\n",
    "        right_data_weights = data_weights[list(np.where(data[feature] == 1)[0])]\n",
    "        \n",
    "        #print feature, len(left_data_weights), len(right_data_weights), sum(left_data_weights), sum(right_data_weights)\n",
    "        \n",
    "        # DIFFERENT HERE\n",
    "        # Calculate the weight of mistakes for left and right sides\n",
    "        ## YOUR CODE HERE\n",
    "        left_weighted_mistakes, left_class = intermediate_node_weighted_mistakes(left_split[target], left_data_weights)\n",
    "        right_weighted_mistakes, right_class = intermediate_node_weighted_mistakes(right_split[target], right_data_weights)\n",
    "        \n",
    "        # DIFFERENT HERE\n",
    "        # Compute weighted error by computing\n",
    "        #  ( [weight of mistakes (left)] + [weight of mistakes (right)] ) / [total weight of all data points]\n",
    "        ## YOUR CODE HERE\n",
    "        error = float(left_weighted_mistakes + right_weighted_mistakes) / float(sum(data_weights))\n",
    "        \n",
    "        #print feature, error, data_weights\n",
    "        \n",
    "        # If this is the best error we have found so far, store the feature and the error\n",
    "        if error < best_error:\n",
    "            best_feature = feature\n",
    "            best_error = error\n",
    "    \n",
    "    # Return the best feature we found\n",
    "    return best_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data = pd.concat([X_train,y_train],axis=1)\n",
    "updated_features = training_data.columns[:-1].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_weights = np.array(len(training_data)* [1.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "example_data_weights = np.array(len(training_data)* [1.5])\n",
    "if best_splitting_feature(training_data, updated_features, target, example_data_weights) == 'term_ 36 months':\n",
    "    print 'Test passed!'\n",
    "else:\n",
    "    print 'Test failed... try again!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['grade', 'term', 'home_ownership', 'emp_length']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Leaf structure\n",
    "#{ \n",
    "#   'is_leaf'            : True/False.\n",
    "#   'prediction'         : Prediction at the leaf node.\n",
    "#   'left'               : (dictionary corresponding to the left tree).\n",
    "#   'right'              : (dictionary corresponding to the right tree).\n",
    "#   'splitting_feature'  : The feature that this node splits on.\n",
    "#}\n",
    "\n",
    "def create_leaf(target_values, data_weights):\n",
    "    \n",
    "    # Create a leaf node\n",
    "    leaf = {'splitting_feature' : None,\n",
    "            'is_leaf': True}\n",
    "    \n",
    "    # Computed weight of mistakes.\n",
    "    # Store the predicted class (1 or -1) in leaf['prediction']\n",
    "    weighted_error, best_class = intermediate_node_weighted_mistakes(target_values, data_weights)\n",
    "    leaf['prediction'] = best_class ## YOUR CODE HERE\n",
    "    \n",
    "    return leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now write a function that learns a weighted decision tree recursively and implements 3 stopping conditions:\n",
    "\n",
    "# - All data points in a node are from the same class.\n",
    "# - No more features to split on.\n",
    "# - Stop growing the tree when the tree depth reaches max_depth.\n",
    "\n",
    "def weighted_decision_tree_create(data, features, target, data_weights, current_depth = 1, max_depth = 10):\n",
    "    remaining_features = features[:] # Make a copy of the features.\n",
    "    target_values = data[target]\n",
    "    print \"--------------------------------------------------------------------\"\n",
    "    print \"Subtree, depth = %s (%s data points).\" % (current_depth, len(target_values))\n",
    "    \n",
    "    # Stopping condition 1. Error is 0.\n",
    "    if intermediate_node_weighted_mistakes(target_values, data_weights)[0] <= 1e-15:\n",
    "        print \"Stopping condition 1 reached.\"                \n",
    "        return create_leaf(target_values, data_weights)\n",
    "    \n",
    "    # Stopping condition 2. No more features.\n",
    "    if remaining_features == []:\n",
    "        print \"Stopping condition 2 reached.\"                \n",
    "        return create_leaf(target_values, data_weights)    \n",
    "    \n",
    "    # Additional stopping condition (limit tree depth)\n",
    "    if current_depth > max_depth:\n",
    "        print \"Reached maximum depth. Stopping for now.\"\n",
    "        return create_leaf(target_values, data_weights)\n",
    "    \n",
    "    # If all the datapoints are the same, splitting_feature will be None. Create a leaf\n",
    "    splitting_feature = best_splitting_feature(data, features, target, data_weights)\n",
    "    #print splitting_feature, remaining_features\n",
    "    remaining_features.remove(splitting_feature)\n",
    "        \n",
    "    left_split = data[data[splitting_feature] == 0]\n",
    "    right_split = data[data[splitting_feature] == 1]\n",
    "    \n",
    "    left_data_weights = data_weights[list(np.where(data[splitting_feature] == 0)[0])]\n",
    "    right_data_weights = data_weights[list(np.where(data[splitting_feature] == 1)[0])]\n",
    "    \n",
    "    print \"Split on feature %s. (%s, %s)\" % (\\\n",
    "              splitting_feature, len(left_split), len(right_split))\n",
    "    \n",
    "    # Create a leaf node if the split is \"perfect\"\n",
    "    if len(left_split) == len(data):\n",
    "        print \"Creating leaf node.\"\n",
    "        return create_leaf(left_split[target], data_weights)\n",
    "    if len(right_split) == len(data):\n",
    "        print \"Creating leaf node.\"\n",
    "        return create_leaf(right_split[target], data_weights)\n",
    "    \n",
    "    # Repeat (recurse) on left and right subtrees\n",
    "    left_tree = weighted_decision_tree_create(\n",
    "        left_split, remaining_features, target, left_data_weights, current_depth + 1, max_depth)\n",
    "    right_tree = weighted_decision_tree_create(\n",
    "        right_split, remaining_features, target, right_data_weights, current_depth + 1, max_depth)\n",
    "    \n",
    "    return {'is_leaf'          : False, \n",
    "            'prediction'       : None,\n",
    "            'splitting_feature': splitting_feature,\n",
    "            'left'             : left_tree, \n",
    "            'right'            : right_tree}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_nodes(tree):\n",
    "    if tree['is_leaf']:\n",
    "        return 1\n",
    "    return 1 + count_nodes(tree['left']) + count_nodes(tree['right'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature term_ 36 months. (9223, 28001)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9223 data points).\n",
      "Split on feature grade_A. (9122, 101)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (9122 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (101 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28001 data points).\n",
      "Split on feature grade_D. (23300, 4701)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (23300 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (4701 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "example_data_weights = np.array([1.0 for i in range(len(training_data))])\n",
    "small_data_decision_tree = weighted_decision_tree_create(training_data, updated_features, target,\n",
    "                                        example_data_weights, max_depth=2)\n",
    "if count_nodes(small_data_decision_tree) == 7:\n",
    "    print 'Test passed!'\n",
    "else:\n",
    "    print 'Test failed... try again!'\n",
    "    print 'Number of nodes found:', count_nodes(small_data_decision_tree)\n",
    "    print 'Number of nodes that should be there: 7' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is_leaf': False,\n",
       " 'left': {'is_leaf': False,\n",
       "  'left': {'is_leaf': True, 'prediction': -1, 'splitting_feature': None},\n",
       "  'prediction': None,\n",
       "  'right': {'is_leaf': True, 'prediction': 1, 'splitting_feature': None},\n",
       "  'splitting_feature': 'grade_A'},\n",
       " 'prediction': None,\n",
       " 'right': {'is_leaf': False,\n",
       "  'left': {'is_leaf': True, 'prediction': 1, 'splitting_feature': None},\n",
       "  'prediction': None,\n",
       "  'right': {'is_leaf': True, 'prediction': -1, 'splitting_feature': None},\n",
       "  'splitting_feature': 'grade_D'},\n",
       " 'splitting_feature': 'term_ 36 months'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_data_decision_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify(tree, x, annotate = False):   \n",
    "    # If the node is a leaf node.\n",
    "    if tree['is_leaf']:\n",
    "        if annotate: \n",
    "            print \"At leaf, predicting %s\" % tree['prediction']\n",
    "        return tree['prediction'] \n",
    "    else:\n",
    "        # Split on feature.\n",
    "        split_feature_value = x[tree['splitting_feature']]\n",
    "        if annotate: \n",
    "            print \"Split on %s = %s\" % (tree['splitting_feature'], split_feature_value)\n",
    "        if split_feature_value == 0:\n",
    "            return classify(tree['left'], x, annotate)\n",
    "        else:\n",
    "            return classify(tree['right'], x, annotate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_classification_error(tree, data):\n",
    "    # Apply the classify(tree, x) to each row in your data\n",
    "    prediction = data.apply(lambda x: classify(tree, x), axis=1)\n",
    "    \n",
    "    # Once you've made the predictions, calculate the classification error\n",
    "    return (prediction != data[target]).sum() / float(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testing_data = pd.concat([X_train,y_train],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>grade_A</th>\n",
       "      <th>grade_B</th>\n",
       "      <th>grade_C</th>\n",
       "      <th>grade_D</th>\n",
       "      <th>grade_E</th>\n",
       "      <th>grade_F</th>\n",
       "      <th>grade_G</th>\n",
       "      <th>term_ 36 months</th>\n",
       "      <th>term_ 60 months</th>\n",
       "      <th>home_ownership_MORTGAGE</th>\n",
       "      <th>...</th>\n",
       "      <th>emp_length_3 years</th>\n",
       "      <th>emp_length_4 years</th>\n",
       "      <th>emp_length_5 years</th>\n",
       "      <th>emp_length_6 years</th>\n",
       "      <th>emp_length_7 years</th>\n",
       "      <th>emp_length_8 years</th>\n",
       "      <th>emp_length_9 years</th>\n",
       "      <th>emp_length_&lt; 1 year</th>\n",
       "      <th>emp_length_n/a</th>\n",
       "      <th>safe_loans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    grade_A  grade_B  grade_C  grade_D  grade_E  grade_F  grade_G  \\\n",
       "1         0        0        1        0        0        0        0   \n",
       "6         0        0        0        0        0        1        0   \n",
       "7         0        1        0        0        0        0        0   \n",
       "10        0        0        1        0        0        0        0   \n",
       "12        0        1        0        0        0        0        0   \n",
       "\n",
       "    term_ 36 months  term_ 60 months  home_ownership_MORTGAGE     ...      \\\n",
       "1                 0                1                        0     ...       \n",
       "6                 0                1                        0     ...       \n",
       "7                 0                1                        0     ...       \n",
       "10                1                0                        0     ...       \n",
       "12                1                0                        0     ...       \n",
       "\n",
       "    emp_length_3 years  emp_length_4 years  emp_length_5 years  \\\n",
       "1                    0                   0                   0   \n",
       "6                    0                   1                   0   \n",
       "7                    0                   0                   0   \n",
       "10                   0                   0                   0   \n",
       "12                   1                   0                   0   \n",
       "\n",
       "    emp_length_6 years  emp_length_7 years  emp_length_8 years  \\\n",
       "1                    0                   0                   0   \n",
       "6                    0                   0                   0   \n",
       "7                    0                   0                   0   \n",
       "10                   0                   0                   0   \n",
       "12                   0                   0                   0   \n",
       "\n",
       "    emp_length_9 years  emp_length_< 1 year  emp_length_n/a  safe_loans  \n",
       "1                    0                    1               0          -1  \n",
       "6                    0                    0               0          -1  \n",
       "7                    0                    1               0          -1  \n",
       "10                   0                    1               0          -1  \n",
       "12                   0                    0               0          -1  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40003761014399314"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_classification_error(small_data_decision_tree, testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature home_ownership_RENT. (20514, 16710)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (20514 data points).\n",
      "Split on feature grade_F. (19613, 901)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (19613 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (901 data points).\n",
      "Stopping condition 1 reached.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (16710 data points).\n",
      "Split on feature grade_D. (13315, 3395)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (13315 data points).\n",
      "Stopping condition 1 reached.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (3395 data points).\n",
      "Stopping condition 1 reached.\n"
     ]
    }
   ],
   "source": [
    "# Assign weights\n",
    "example_data_weights = np.array([1.] * 10 + [0.]*(len(train_data) - 20) + [1.] * 10)\n",
    "# Train a weighted decision tree model.\n",
    "small_data_decision_tree_subset_20 = weighted_decision_tree_create(training_data, updated_features, target,\n",
    "                         example_data_weights, max_depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification errors: 0.481248656781 percent\n"
     ]
    }
   ],
   "source": [
    "print 'Classification errors: %s percent' % evaluate_classification_error(small_data_decision_tree_subset_20, training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pred = training_data.apply(lambda x: classify(small_data_decision_tree_subset_20, x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_stump(tree, name = 'root'):\n",
    "    split_name = tree['splitting_feature'] # split_name is something like 'term. 36 months'\n",
    "    if split_name is None:\n",
    "        print \"(leaf, label: %s)\" % tree['prediction']\n",
    "        return None\n",
    "    split_feature, split_value = split_name.split('_')\n",
    "    print '                       %s' % name\n",
    "    print '         |---------------|----------------|'\n",
    "    print '         |                                |'\n",
    "    print '         |                                |'\n",
    "    print '         |                                |'\n",
    "    print '  [{0} == 0]               [{0} == 1]    '.format(split_name)\n",
    "    print '         |                                |'\n",
    "    print '         |                                |'\n",
    "    print '         |                                |'\n",
    "    print '    (%s)                         (%s)' \\\n",
    "        % (('leaf, label: ' + str(tree['left']['prediction']) if tree['left']['is_leaf'] else 'subtree'),\n",
    "           ('leaf, label: ' + str(tree['right']['prediction']) if tree['right']['is_leaf'] else 'subtree'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is_leaf': False,\n",
       " 'left': {'is_leaf': False,\n",
       "  'left': {'is_leaf': True, 'prediction': 1, 'splitting_feature': None},\n",
       "  'prediction': None,\n",
       "  'right': {'is_leaf': True, 'prediction': -1, 'splitting_feature': None},\n",
       "  'splitting_feature': 'grade_F'},\n",
       " 'prediction': None,\n",
       " 'right': {'is_leaf': False,\n",
       "  'left': {'is_leaf': True, 'prediction': -1, 'splitting_feature': None},\n",
       "  'prediction': None,\n",
       "  'right': {'is_leaf': True, 'prediction': 1, 'splitting_feature': None},\n",
       "  'splitting_feature': 'grade_D'},\n",
       " 'splitting_feature': 'home_ownership_RENT'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_data_decision_tree_subset_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       home_ownership_RENT\n",
      "         |---------------|----------------|\n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "  [grade_F == 0]               [grade_F == 1]    \n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "    (leaf, label: 1)                         (leaf, label: -1)\n"
     ]
    }
   ],
   "source": [
    "print_stump(small_data_decision_tree_subset_20['left'], small_data_decision_tree_subset_20['splitting_feature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Implementing Adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Implementing your own Adaboost (on decision stumps)\n",
    "Now that we have a weighted decision tree working, it takes only a bit of work to implement Adaboost. For the sake of simplicity, let us stick with decision tree stumps by training trees with max_depth=1.\n",
    "Recall from the lecture the procedure for Adaboost:\n",
    "1. Start with unweighted data with $\\alpha_j = 1$\n",
    "2. For t = 1,...T:\n",
    "Learn $f_t(x)$ with data weights $\\alpha_j$\n",
    "Compute coefficient $\\hat{w}_t$: $$\\hat{w}_t = \\frac{1}{2}\\ln{\\left(\\frac{1- \\mbox{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})}{\\mbox{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})}\\right)}$$\n",
    "Re-compute weights $\\alpha_j$: $$\\alpha_j \\gets \\begin{cases}\n",
    " \\alpha_j \\exp{(-\\hat{w}_t)} &amp; \\text{ if }f_t(x_j) = y_j\\\\\n",
    " \\alpha_j \\exp{(\\hat{w}_t)} &amp; \\text{ if }f_t(x_j) \\neq y_j\n",
    " \\end{cases}$$\n",
    "Normalize weights $\\alpha_j$: $$\\alpha_j \\gets \\frac{\\alpha_j}{\\sum_{i=1}^{N}{\\alpha_i}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "from math import exp\n",
    "\n",
    "def adaboost_with_tree_stumps(data, features, target, num_tree_stumps):\n",
    "    # start with unweighted data\n",
    "    alpha = np.array([1.]*len(data))\n",
    "    weights = []\n",
    "    tree_stumps = []\n",
    "    target_values = data[target]\n",
    "    \n",
    "    for t in xrange(num_tree_stumps):\n",
    "        print '====================================================='\n",
    "        print 'Adaboost Iteration %d' % t\n",
    "        print '====================================================='        \n",
    "        # Learn a weighted decision tree stump. Use max_depth=1\n",
    "        tree_stump = weighted_decision_tree_create(data, features, target, data_weights=alpha, max_depth=1)\n",
    "        tree_stumps.append(tree_stump)\n",
    "        \n",
    "        # Make predictions\n",
    "        #predictions = data.apply(lambda x: classify(tree_stump, x))\n",
    "        predictions = data.apply(lambda x: classify(tree_stump, x), axis=1)\n",
    "        \n",
    "        # Produce a Boolean array indicating whether\n",
    "        # each data point was correctly classified\n",
    "        is_correct = predictions == target_values\n",
    "        is_wrong   = predictions != target_values\n",
    "        \n",
    "        # Compute weighted error\n",
    "        # YOUR CODE HERE\n",
    "        #print len(is_wrong[is_wrong == True]), len(alpha[np.where(is_wrong)])\n",
    "        weighted_error = sum(alpha[np.where(is_wrong)])/sum(alpha)\n",
    "        \n",
    "        # Compute model coefficient using weighted error\n",
    "        # YOUR CODE HERE\n",
    "        weight = 1./2. * log((1 - weighted_error)/weighted_error)\n",
    "        weights.append(weight)\n",
    "        \n",
    "        # Adjust weights on data point\n",
    "        adjustment = np.array(is_correct.apply(lambda is_correct : exp(-weight) if is_correct else exp(weight)))\n",
    "        \n",
    "        # Scale alpha by multiplying by adjustment\n",
    "        # Then normalize data points weights\n",
    "        ## YOUR CODE HERE \n",
    "        #print alpha\n",
    "        #print adjustment\n",
    "        alpha = (alpha*adjustment)/float(sum(alpha))\n",
    "    \n",
    "    return weights, tree_stumps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "Adaboost Iteration 0\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature term_ 36 months. (9223, 28001)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9223 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28001 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 1\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_A. (32094, 5130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (32094 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5130 data points).\n",
      "Reached maximum depth. Stopping for now.\n"
     ]
    }
   ],
   "source": [
    "stump_weights, tree_stumps = adaboost_with_tree_stumps(training_data, updated_features, target, num_tree_stumps=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Reminders\n",
    "\n",
    "    Stump weights (ŵ) and data point weights (α) are two different concepts.\n",
    "    Stump weights (ŵ) tell you how important each stump is while making predictions with the entire boosted ensemble.\n",
    "    Data point weights (α) tell you how important each data point is while training a decision stump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.15802933659263743, 0.17682363293605327]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stump_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'is_leaf': False,\n",
       "  'left': {'is_leaf': True, 'prediction': -1, 'splitting_feature': None},\n",
       "  'prediction': None,\n",
       "  'right': {'is_leaf': True, 'prediction': 1, 'splitting_feature': None},\n",
       "  'splitting_feature': 'term_ 36 months'},\n",
       " {'is_leaf': False,\n",
       "  'left': {'is_leaf': True, 'prediction': -1, 'splitting_feature': None},\n",
       "  'prediction': None,\n",
       "  'right': {'is_leaf': True, 'prediction': 1, 'splitting_feature': None},\n",
       "  'splitting_feature': 'grade_A'}]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_stumps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "Adaboost Iteration 0\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature term_ 36 months. (9223, 28001)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9223 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28001 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 1\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_A. (32094, 5130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (32094 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 2\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_D. (30465, 6759)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (30465 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (6759 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 3\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature home_ownership_MORTGAGE. (19846, 17378)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (19846 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (17378 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 4\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_B. (26858, 10366)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (26858 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (10366 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 5\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_E. (33815, 3409)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (33815 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3409 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 6\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_A. (32094, 5130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (32094 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 7\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_F. (35512, 1712)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35512 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1712 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 8\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_A. (32094, 5130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (32094 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 9\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature emp_length_n/a. (35781, 1443)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35781 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1443 data points).\n",
      "Reached maximum depth. Stopping for now.\n"
     ]
    }
   ],
   "source": [
    "stump_weights, tree_stumps = adaboost_with_tree_stumps(training_data, updated_features, target, num_tree_stumps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.15802933659263743,\n",
       " 0.17682363293605327,\n",
       " 0.09311888971195705,\n",
       " 0.0728888552581495,\n",
       " 0.06706306914131716,\n",
       " 0.06456916961613322,\n",
       " 0.05456055779221647,\n",
       " 0.04351093673354489,\n",
       " 0.028988711500059067,\n",
       " 0.0259625096913776]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stump_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'is_leaf': False,\n",
       "  'left': {'is_leaf': True, 'prediction': -1, 'splitting_feature': None},\n",
       "  'prediction': None,\n",
       "  'right': {'is_leaf': True, 'prediction': 1, 'splitting_feature': None},\n",
       "  'splitting_feature': 'term_ 36 months'},\n",
       " {'is_leaf': False,\n",
       "  'left': {'is_leaf': True, 'prediction': -1, 'splitting_feature': None},\n",
       "  'prediction': None,\n",
       "  'right': {'is_leaf': True, 'prediction': 1, 'splitting_feature': None},\n",
       "  'splitting_feature': 'grade_A'},\n",
       " {'is_leaf': False,\n",
       "  'left': {'is_leaf': True, 'prediction': 1, 'splitting_feature': None},\n",
       "  'prediction': None,\n",
       "  'right': {'is_leaf': True, 'prediction': -1, 'splitting_feature': None},\n",
       "  'splitting_feature': 'grade_D'},\n",
       " {'is_leaf': False,\n",
       "  'left': {'is_leaf': True, 'prediction': -1, 'splitting_feature': None},\n",
       "  'prediction': None,\n",
       "  'right': {'is_leaf': True, 'prediction': 1, 'splitting_feature': None},\n",
       "  'splitting_feature': 'home_ownership_MORTGAGE'},\n",
       " {'is_leaf': False,\n",
       "  'left': {'is_leaf': True, 'prediction': -1, 'splitting_feature': None},\n",
       "  'prediction': None,\n",
       "  'right': {'is_leaf': True, 'prediction': 1, 'splitting_feature': None},\n",
       "  'splitting_feature': 'grade_B'},\n",
       " {'is_leaf': False,\n",
       "  'left': {'is_leaf': True, 'prediction': 1, 'splitting_feature': None},\n",
       "  'prediction': None,\n",
       "  'right': {'is_leaf': True, 'prediction': -1, 'splitting_feature': None},\n",
       "  'splitting_feature': 'grade_E'},\n",
       " {'is_leaf': False,\n",
       "  'left': {'is_leaf': True, 'prediction': -1, 'splitting_feature': None},\n",
       "  'prediction': None,\n",
       "  'right': {'is_leaf': True, 'prediction': 1, 'splitting_feature': None},\n",
       "  'splitting_feature': 'grade_A'},\n",
       " {'is_leaf': False,\n",
       "  'left': {'is_leaf': True, 'prediction': 1, 'splitting_feature': None},\n",
       "  'prediction': None,\n",
       "  'right': {'is_leaf': True, 'prediction': -1, 'splitting_feature': None},\n",
       "  'splitting_feature': 'grade_F'},\n",
       " {'is_leaf': False,\n",
       "  'left': {'is_leaf': True, 'prediction': -1, 'splitting_feature': None},\n",
       "  'prediction': None,\n",
       "  'right': {'is_leaf': True, 'prediction': 1, 'splitting_feature': None},\n",
       "  'splitting_feature': 'grade_A'},\n",
       " {'is_leaf': False,\n",
       "  'left': {'is_leaf': True, 'prediction': 1, 'splitting_feature': None},\n",
       "  'prediction': None,\n",
       "  'right': {'is_leaf': True, 'prediction': -1, 'splitting_feature': None},\n",
       "  'splitting_feature': 'emp_length_n/a'}]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_stumps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_adaboost(stump_weights, tree_stumps, data):\n",
    "    scores = np.array([0.]*len(data))\n",
    "    \n",
    "    for i, tree_stump in enumerate(tree_stumps):\n",
    "        predictions = data.apply(lambda x: classify(tree_stump, x), axis=1)\n",
    "        \n",
    "        # Accumulate predictions on scores array\n",
    "        # YOUR CODE HERE\n",
    "        weighted_predictions = stump_weights[i] * predictions\n",
    "        #print weighted_predictions.shape, scores.shape, predictions.shape\n",
    "        #print predictions.iloc[:10]\n",
    "        scores = scores + weighted_predictions\n",
    "        #print scores.shape\n",
    "        \n",
    "    #return scores.apply(lambda score : +1 if score > 0 else -1)\n",
    "    return scores.apply(lambda score : +1 if score > 0 else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "Adaboost Iteration 0\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature term_ 36 months. (9223, 28001)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9223 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28001 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 1\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_A. (32094, 5130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (32094 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 2\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_D. (30465, 6759)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (30465 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (6759 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 3\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature home_ownership_MORTGAGE. (19846, 17378)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (19846 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (17378 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 4\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_B. (26858, 10366)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (26858 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (10366 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 5\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_E. (33815, 3409)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (33815 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3409 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 6\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_A. (32094, 5130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (32094 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 7\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_F. (35512, 1712)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35512 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1712 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 8\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_A. (32094, 5130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (32094 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 9\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature emp_length_n/a. (35781, 1443)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35781 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1443 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 10\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_D. (30465, 6759)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (30465 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (6759 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 11\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_B. (26858, 10366)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (26858 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (10366 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 12\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature emp_length_n/a. (35781, 1443)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35781 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1443 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 13\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature emp_length_4 years. (34593, 2631)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (34593 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (2631 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 14\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature emp_length_n/a. (35781, 1443)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35781 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1443 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 15\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_C. (27812, 9412)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (27812 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9412 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 16\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_A. (32094, 5130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (32094 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 17\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_F. (35512, 1712)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35512 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1712 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 18\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature term_ 36 months. (9223, 28001)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9223 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28001 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 19\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_B. (26858, 10366)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (26858 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (10366 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 20\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature emp_length_n/a. (35781, 1443)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35781 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1443 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 21\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_D. (30465, 6759)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (30465 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (6759 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 22\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_F. (35512, 1712)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35512 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1712 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 23\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_A. (32094, 5130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (32094 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 24\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature emp_length_n/a. (35781, 1443)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35781 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1443 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 25\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature emp_length_2 years. (33652, 3572)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (33652 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3572 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 26\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_F. (35512, 1712)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35512 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1712 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 27\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature home_ownership_OWN. (34149, 3075)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (34149 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3075 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 28\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature emp_length_n/a. (35781, 1443)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35781 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1443 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 29\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_C. (27812, 9412)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (27812 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9412 data points).\n",
      "Reached maximum depth. Stopping for now.\n"
     ]
    }
   ],
   "source": [
    "stump_weights, tree_stumps = adaboost_with_tree_stumps(training_data, updated_features, target, num_tree_stumps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, training error = 0.421636578551\n",
      "Iteration 2, training error = 0.433430045132\n",
      "Iteration 3, training error = 0.400037610144\n",
      "Iteration 4, training error = 0.400037610144\n",
      "Iteration 5, training error = 0.384724908661\n",
      "Iteration 6, training error = 0.384617451107\n",
      "Iteration 7, training error = 0.382763808296\n",
      "Iteration 8, training error = 0.384617451107\n",
      "Iteration 9, training error = 0.382763808296\n",
      "Iteration 10, training error = 0.384483129164\n",
      "Iteration 11, training error = 0.382736943907\n",
      "Iteration 12, training error = 0.381447453256\n",
      "Iteration 13, training error = 0.381528046422\n",
      "Iteration 14, training error = 0.380560928433\n",
      "Iteration 15, training error = 0.380507199656\n",
      "Iteration 16, training error = 0.378223726628\n",
      "Iteration 17, training error = 0.378277455405\n",
      "Iteration 18, training error = 0.378411777348\n",
      "Iteration 19, training error = 0.378062540297\n",
      "Iteration 20, training error = 0.378761014399\n",
      "Iteration 21, training error = 0.379566946056\n",
      "Iteration 22, training error = 0.378895336342\n",
      "Iteration 23, training error = 0.378895336342\n",
      "Iteration 24, training error = 0.378761014399\n",
      "Iteration 25, training error = 0.378895336342\n",
      "Iteration 26, training error = 0.378975929508\n",
      "Iteration 27, training error = 0.379110251451\n",
      "Iteration 28, training error = 0.378922200731\n",
      "Iteration 29, training error = 0.379029658285\n",
      "Iteration 30, training error = 0.378734150011\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "error_all = []\n",
    "for n in xrange(1, 31):\n",
    "    predictions = predict_adaboost(stump_weights[:n], tree_stumps[:n], training_data)\n",
    "    error = 1.0 - accuracy_score(training_data[target], predictions)\n",
    "    error_all.append(error)\n",
    "    print \"Iteration %s, training error = %s\" % (n, error_all[n-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdoAAAFgCAYAAAActbi8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nK5EDrZmZWYkcaM3MzErkQGtmZlai/weTvRjzWzDgJwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5f1dd6ced0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "plt.rcParams['figure.figsize'] = 7, 5\n",
    "plt.plot(range(1,31), error_all, '-', linewidth=4.0, label='Training error')\n",
    "plt.title('Performance of Adaboost ensemble')\n",
    "plt.xlabel('# of iterations')\n",
    "plt.ylabel('Classification error')\n",
    "plt.legend(loc='best', prop={'size':15})\n",
    "\n",
    "plt.rcParams.update({'font.size': 16})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, training error = 0.421636578551\n",
      "Iteration 2, training error = 0.433430045132\n",
      "Iteration 3, training error = 0.400037610144\n",
      "Iteration 4, training error = 0.400037610144\n",
      "Iteration 5, training error = 0.384724908661\n",
      "Iteration 6, training error = 0.384617451107\n",
      "Iteration 7, training error = 0.382763808296\n",
      "Iteration 8, training error = 0.384617451107\n",
      "Iteration 9, training error = 0.382763808296\n",
      "Iteration 10, training error = 0.384483129164\n",
      "Iteration 11, training error = 0.382736943907\n",
      "Iteration 12, training error = 0.381447453256\n",
      "Iteration 13, training error = 0.381528046422\n",
      "Iteration 14, training error = 0.380560928433\n",
      "Iteration 15, training error = 0.380507199656\n",
      "Iteration 16, training error = 0.378223726628\n",
      "Iteration 17, training error = 0.378277455405\n",
      "Iteration 18, training error = 0.378411777348\n",
      "Iteration 19, training error = 0.378062540297\n",
      "Iteration 20, training error = 0.378761014399\n",
      "Iteration 21, training error = 0.379566946056\n",
      "Iteration 22, training error = 0.378895336342\n",
      "Iteration 23, training error = 0.378895336342\n",
      "Iteration 24, training error = 0.378761014399\n",
      "Iteration 25, training error = 0.378895336342\n",
      "Iteration 26, training error = 0.378975929508\n",
      "Iteration 27, training error = 0.379110251451\n",
      "Iteration 28, training error = 0.378922200731\n",
      "Iteration 29, training error = 0.379029658285\n",
      "Iteration 30, training error = 0.378734150011\n"
     ]
    }
   ],
   "source": [
    "test_error_all = []\n",
    "for n in xrange(1, 31):\n",
    "    predictions = predict_adaboost(stump_weights[:n], tree_stumps[:n], testing_data)\n",
    "    test_error = 1.0 - accuracy_score(testing_data[target], predictions)\n",
    "    test_error_all.append(test_error)\n",
    "    print \"Iteration %s, training error = %s\" % (n, test_error_all[n-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeIAAAFSCAYAAAAuI9zWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBNsGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5f16f717d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = 7, 5\n",
    "plt.plot(range(1,31), error_all, '-', linewidth=4.0, label='Training error')\n",
    "plt.plot(range(1,31), test_error_all, '-', linewidth=4.0, label='Test error')\n",
    "\n",
    "plt.title('Performance of Adaboost ensemble')\n",
    "plt.xlabel('# of iterations')\n",
    "plt.ylabel('Classification error')\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.legend(loc='best', prop={'size':15})\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
